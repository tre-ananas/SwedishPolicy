{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ryanh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ryanh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Packages and Model Commands\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "import torch\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Swedish stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words_swedish = set(stopwords.words('swedish'))\n",
    "\n",
    "# Download the NLTK sentence tokenizer data\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer from Recorded Future\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"RecordedFuture/Swedish-Sentiment-Fear\")\n",
    "classifier_fear= BertForSequenceClassification.from_pretrained(\"RecordedFuture/Swedish-Sentiment-Fear\")\n",
    "classifier_violence = BertForSequenceClassification.from_pretrained(\"RecordedFuture/Swedish-Sentiment-Violence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'unclassified_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ryanh\\Desktop\\Capstone\\Code\\Python\\SwedishGovernmentDocuments\\SwedishPolicy\\GD6_Sentiment_Analysis.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ryanh/Desktop/Capstone/Code/Python/SwedishGovernmentDocuments/SwedishPolicy/GD6_Sentiment_Analysis.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load Data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ryanh/Desktop/Capstone/Code/Python/SwedishGovernmentDocuments/SwedishPolicy/GD6_Sentiment_Analysis.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39munclassified_data.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ryanh\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ryanh\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ryanh\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\ryanh\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\ryanh\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\ryanh\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\ryanh\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'unclassified_data.csv'"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv(\"unclassified_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows without text data\n",
    "data = data[data['Text'] != 'NO CONTENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename index column\n",
    "data.rename(columns={'Unnamed: 0': 'Index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Document Type</th>\n",
       "      <th>Source</th>\n",
       "      <th>PDF Indicator</th>\n",
       "      <th>Text</th>\n",
       "      <th>Content Page</th>\n",
       "      <th>PDF Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>02 november 2023</td>\n",
       "      <td>Remiss</td>\n",
       "      <td>Utbildningsdepartementet</td>\n",
       "      <td>1</td>\n",
       "      <td>2023 -04-26  U2023/01467          ...</td>\n",
       "      <td>https://www.regeringen.se/remisser/2023/11/inb...</td>\n",
       "      <td>https://www.regeringen.se/contentassets/c9981c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index              Date Document Type                    Source  \\\n",
       "0      0  02 november 2023        Remiss  Utbildningsdepartementet   \n",
       "\n",
       "   PDF Indicator                                               Text  \\\n",
       "0              1              2023 -04-26  U2023/01467          ...   \n",
       "\n",
       "                                        Content Page  \\\n",
       "0  https://www.regeringen.se/remisser/2023/11/inb...   \n",
       "\n",
       "                                            PDF Link  \n",
       "0  https://www.regeringen.se/contentassets/c9981c...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows of data into a test set\n",
    "unprocessed_data = data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Document Type</th>\n",
       "      <th>Source</th>\n",
       "      <th>PDF Indicator</th>\n",
       "      <th>Text</th>\n",
       "      <th>Content Page</th>\n",
       "      <th>PDF Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>02 november 2023</td>\n",
       "      <td>Remiss</td>\n",
       "      <td>Utbildningsdepartementet</td>\n",
       "      <td>1</td>\n",
       "      <td>2023 -04-26  U2023/01467          ...</td>\n",
       "      <td>https://www.regeringen.se/remisser/2023/11/inb...</td>\n",
       "      <td>https://www.regeringen.se/contentassets/c9981c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index              Date Document Type                    Source  \\\n",
       "0      0  02 november 2023        Remiss  Utbildningsdepartementet   \n",
       "\n",
       "   PDF Indicator                                               Text  \\\n",
       "0              1              2023 -04-26  U2023/01467          ...   \n",
       "\n",
       "                                        Content Page  \\\n",
       "0  https://www.regeringen.se/remisser/2023/11/inb...   \n",
       "\n",
       "                                            PDF Link  \n",
       "0  https://www.regeringen.se/contentassets/c9981c...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unprocessed_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up lists for sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store probabilities, texts, and IDs for both classifiers\n",
    "probabilities_fear_list = []\n",
    "probabilities_violence_list = []\n",
    "texts_list = []\n",
    "ids_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts and IDs from the desired columns (assuming column names are \"text_column\" and \"id_column\")\n",
    "text_entries = unprocessed_data['Text'].tolist()\n",
    "ids = unprocessed_data['Index'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts: 100%|██████████| 5/5 [00:49<00:00,  9.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for text, text_id in tqdm(zip(text_entries, ids), total=len(text_entries), desc=\"Processing Texts\"):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        # Calculate probabilities for classifier_fear\n",
    "        outputs_fear = classifier_fear(**inputs)\n",
    "        probabilities_fear = torch.nn.functional.softmax(outputs_fear.logits, dim=1).tolist()[0]\n",
    "        probabilities_fear_list.append(probabilities_fear)\n",
    "        \n",
    "        # Calculate probabilities for classifier_violence\n",
    "        outputs_violence = classifier_violence(**inputs)  # Use classifier_violence here\n",
    "        probabilities_violence = torch.nn.functional.softmax(outputs_violence.logits, dim=1).tolist()[0]\n",
    "        probabilities_violence_list.append(probabilities_violence)\n",
    "        \n",
    "        texts_list.append(sentence)\n",
    "        ids_list.append(text_id)\n",
    "\n",
    "# Now, probabilities_fear_list and probabilities_violence_list contain probabilities\n",
    "# for classifier_fear and classifier_violence respectively, for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the classified_sentence_data DataFrame\n",
    "classified_sentence_data = pd.DataFrame({\n",
    "    'Fear Class 0 Probability': [item[0] for item in probabilities_fear_list],\n",
    "    'Fear Class 1 Probability': [item[1] for item in probabilities_fear_list],\n",
    "    'Fear Class 2 Probability': [item[2] for item in probabilities_fear_list],\n",
    "    'Violence Class 0 Probability': [item[0] for item in probabilities_violence_list],\n",
    "    'Violence Class 1 Probability': [item[1] for item in probabilities_violence_list],\n",
    "    'Violence Class 2 Probability': [item[2] for item in probabilities_violence_list],\n",
    "    'Text': texts_list,\n",
    "    'ID': ids_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging additional columns from unprocessed_data based on 'Index' and 'ID'\n",
    "classified_sentence_data = pd.merge(classified_sentence_data, unprocessed_data[['Index', 'Date', 'Document Type', 'Source', 'PDF Indicator', 'Content Page', 'PDF Link']], \n",
    "                                    left_on='ID', right_on='Index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the redundant 'Index' column\n",
    "classified_sentence_data.drop(columns=['Index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering the columns\n",
    "classified_sentence_data = classified_sentence_data[['ID', 'Date',\n",
    "                                                     'Fear Class 0 Probability', 'Fear Class 1 Probability', 'Fear Class 2 Probability',\n",
    "                                                     'Violence Class 0 Probability', 'Violence Class 1 Probability', 'Violence Class 2 Probability',\n",
    "                                                     'Document Type', 'Source', 'PDF Indicator', 'Text', 'Content Page', 'PDF Link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Fear Class 0 Probability</th>\n",
       "      <th>Fear Class 1 Probability</th>\n",
       "      <th>Fear Class 2 Probability</th>\n",
       "      <th>Violence Class 0 Probability</th>\n",
       "      <th>Violence Class 1 Probability</th>\n",
       "      <th>Violence Class 2 Probability</th>\n",
       "      <th>Document Type</th>\n",
       "      <th>Source</th>\n",
       "      <th>PDF Indicator</th>\n",
       "      <th>Text</th>\n",
       "      <th>Content Page</th>\n",
       "      <th>PDF Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>02 november 2023</td>\n",
       "      <td>0.996882</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.999418</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>Remiss</td>\n",
       "      <td>Utbildningsdepartementet</td>\n",
       "      <td>1</td>\n",
       "      <td>2023 -04-26  U2023/01467          ...</td>\n",
       "      <td>https://www.regeringen.se/remisser/2023/11/inb...</td>\n",
       "      <td>https://www.regeringen.se/contentassets/c9981c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID              Date  Fear Class 0 Probability  Fear Class 1 Probability  \\\n",
       "0   0  02 november 2023                  0.996882                  0.002071   \n",
       "\n",
       "   Fear Class 2 Probability  Violence Class 0 Probability  \\\n",
       "0                  0.001046                      0.999418   \n",
       "\n",
       "   Violence Class 1 Probability  Violence Class 2 Probability Document Type  \\\n",
       "0                      0.000276                      0.000306        Remiss   \n",
       "\n",
       "                     Source  PDF Indicator  \\\n",
       "0  Utbildningsdepartementet              1   \n",
       "\n",
       "                                                Text  \\\n",
       "0              2023 -04-26  U2023/01467          ...   \n",
       "\n",
       "                                        Content Page  \\\n",
       "0  https://www.regeringen.se/remisser/2023/11/inb...   \n",
       "\n",
       "                                            PDF Link  \n",
       "0  https://www.regeringen.se/contentassets/c9981c...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_sentence_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify articles by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group result_df by 'ID'\n",
    "classified_sentence_data_grouped = classified_sentence_data.groupby('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store data for the new DataFrame for fear probabilities\n",
    "new_ids = []\n",
    "new_texts = []\n",
    "new_fear_class_0_probs = []\n",
    "new_fear_class_1_probs = []\n",
    "new_fear_class_2_probs = []\n",
    "new_violence_class_0_probs = []\n",
    "new_violence_class_1_probs = []\n",
    "new_violence_class_2_probs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|██████████| 5/5 [00:00<00:00, 180.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fear Classification\n",
    "# Iterate through grouped_df\n",
    "for id, group in tqdm(classified_sentence_data_grouped, desc=\"Processing Groups\"):\n",
    "    num_rows = len(group)\n",
    "    # If there are less than 10 rows, select the row with the lowest Class 0 probability\n",
    "    if num_rows < 10:\n",
    "        min_index = group['Fear Class 0 Probability'].idxmin()\n",
    "        selected_row = group.loc[min_index]\n",
    "        new_ids.append(id)\n",
    "        new_texts.append(selected_row['Text'])\n",
    "        new_fear_class_0_probs.append(selected_row['Fear Class 0 Probability'])\n",
    "        new_fear_class_1_probs.append(selected_row['Fear Class 1 Probability'])\n",
    "        new_fear_class_2_probs.append(selected_row['Fear Class 2 Probability'])\n",
    "    # If there are 10-49 rows, select the two rows with the lowest Class 0 probabilities\n",
    "    elif 10 <= num_rows < 50:\n",
    "        selected_rows = group.nsmallest(2, 'Fear Class 0 Probability')\n",
    "        avg_class_0_prob = selected_rows['Fear Class 0 Probability'].mean()\n",
    "        avg_class_1_prob = selected_rows['Fear Class 1 Probability'].mean()\n",
    "        avg_class_2_prob = selected_rows['Fear Class 2 Probability'].mean()\n",
    "        new_ids.append(id)\n",
    "        new_texts.append(selected_rows['Text'].values[0])  # Select the text from the first row\n",
    "        new_fear_class_0_probs.append(avg_class_0_prob)\n",
    "        new_fear_class_1_probs.append(avg_class_1_prob)\n",
    "        new_fear_class_2_probs.append(avg_class_2_prob)\n",
    "    # If there are 50-99 rows, select the three rows with the lowest Class 0 probabilities\n",
    "    elif 50 <= num_rows < 100:\n",
    "        selected_rows = group.nsmallest(3, 'Fear Class 0 Probability')\n",
    "        avg_class_0_prob = selected_rows['Fear Class 0 Probability'].mean()\n",
    "        avg_class_1_prob = selected_rows['Fear Class 1 Probability'].mean()\n",
    "        avg_class_2_prob = selected_rows['Fear Class 2 Probability'].mean()\n",
    "        new_ids.append(id)\n",
    "        new_texts.append(selected_rows['Text'].values[0])  # Select the text from the first row\n",
    "        new_fear_class_0_probs.append(avg_class_0_prob)\n",
    "        new_fear_class_1_probs.append(avg_class_1_prob)\n",
    "        new_fear_class_2_probs.append(avg_class_2_prob)\n",
    "    # If there are 100-199 rows, select the four rows with the lowest Class 0 probabilities\n",
    "    elif 100 <= num_rows < 200:\n",
    "        selected_rows = group.nsmallest(4, 'Fear Class 0 Probability')\n",
    "        avg_class_0_prob = selected_rows['Fear Class 0 Probability'].mean()\n",
    "        avg_class_1_prob = selected_rows['Fear Class 1 Probability'].mean()\n",
    "        avg_class_2_prob = selected_rows['Fear Class 2 Probability'].mean()\n",
    "        new_ids.append(id)\n",
    "        new_texts.append(selected_rows['Text'].values[0])  # Select the text from the first row\n",
    "        new_fear_class_0_probs.append(avg_class_0_prob)\n",
    "        new_fear_class_1_probs.append(avg_class_1_prob)\n",
    "        new_fear_class_2_probs.append(avg_class_2_prob)\n",
    "    # If there are 200 or more rows, select the five rows with the lowest Class 0 probabilities\n",
    "    else:\n",
    "        selected_rows = group.nsmallest(5, 'Fear Class 0 Probability')\n",
    "        avg_class_0_prob = selected_rows['Fear Class 0 Probability'].mean()\n",
    "        avg_class_1_prob = selected_rows['Fear Class 1 Probability'].mean()\n",
    "        avg_class_2_prob = selected_rows['Fear Class 2 Probability'].mean()\n",
    "        new_ids.append(id)\n",
    "        new_texts.append(selected_rows['Text'].values[0])  # Select the text from the first row\n",
    "        new_fear_class_0_probs.append(avg_class_0_prob)\n",
    "        new_fear_class_1_probs.append(avg_class_1_prob)\n",
    "        new_fear_class_2_probs.append(avg_class_2_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store data for the new DataFrame for violence probabilities\n",
    "new_ids = []\n",
    "new_texts = []\n",
    "new_violence_class_0_probs = []\n",
    "new_violence_class_1_probs = []\n",
    "new_violence_class_2_probs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|██████████| 5/5 [00:00<00:00, 430.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Violence Classification\n",
    "# Iterate through grouped_df\n",
    "for id, group in tqdm(classified_sentence_data_grouped, desc=\"Processing Groups\"):\n",
    "    num_rows = len(group)\n",
    "    # If there are less than 10 rows, select the row with the lowest Class 0 probability\n",
    "    if num_rows < 10:\n",
    "        min_index = group['Violence Class 0 Probability'].idxmin()\n",
    "        selected_row = group.loc[min_index]\n",
    "        new_ids.append(id)\n",
    "        new_texts.append(selected_row['Text'])\n",
    "        new_violence_class_0_probs.append(selected_row['Violence Class 0 Probability'])\n",
    "        new_violence_class_1_probs.append(selected_row['Violence Class 1 Probability'])\n",
    "        new_violence_class_2_probs.append(selected_row['Violence Class 2 Probability'])\n",
    "    # If there are 10-49 rows, select the two rows with the lowest Class 0 probabilities\n",
    "    elif 10 <= num_rows < 50:\n",
    "        selected_rows = group.nsmallest(2, 'Violence Class 0 Probability')\n",
    "        avg_class_0_prob = selected_rows['Violence Class 0 Probability'].mean()\n",
    "        avg_class_1_prob = selected_rows['Violence Class 1 Probability'].mean()\n",
    "        avg_class_2_prob = selected_rows['Violence Class 2 Probability'].mean()\n",
    "        new_ids.append(id)\n",
    "        new_texts.append(selected_rows['Text'].values[0])  # Select the text from the first row\n",
    "        new_violence_class_0_probs.append(avg_class_0_prob)\n",
    "        new_violence_class_1_probs.append(avg_class_1_prob)\n",
    "        new_violence_class_2_probs.append(avg_class_2_prob)\n",
    "    # If there are 50-99 rows, select the three rows with the lowest Class 0 probabilities\n",
    "    elif 50 <= num_rows < 100:\n",
    "        selected_rows = group.nsmallest(3, 'Violence Class 0 Probability')\n",
    "        avg_class_0_prob = selected_rows['Violence Class 0 Probability'].mean()\n",
    "        avg_class_1_prob = selected_rows['Violence Class 1 Probability'].mean()\n",
    "        avg_class_2_prob = selected_rows['Violence Class 2 Probability'].mean()\n",
    "        new_ids.append(id)\n",
    "        new_texts.append(selected_rows['Text'].values[0])  # Select the text from the first row\n",
    "        new_violence_class_0_probs.append(avg_class_0_prob)\n",
    "        new_violence_class_1_probs.append(avg_class_1_prob)\n",
    "        new_violence_class_2_probs.append(avg_class_2_prob)\n",
    "    # If there are 100-199 rows, select the four rows with the lowest Class 0 probabilities\n",
    "    elif 100 <= num_rows < 200:\n",
    "        selected_rows = group.nsmallest(4, 'Violence Class 0 Probability')\n",
    "        avg_class_0_prob = selected_rows['Violence Class 0 Probability'].mean()\n",
    "        avg_class_1_prob = selected_rows['Violence Class 1 Probability'].mean()\n",
    "        avg_class_2_prob = selected_rows['Violence Class 2 Probability'].mean()\n",
    "        new_ids.append(id)\n",
    "        new_texts.append(selected_rows['Text'].values[0])  # Select the text from the first row\n",
    "        new_violence_class_0_probs.append(avg_class_0_prob)\n",
    "        new_violence_class_1_probs.append(avg_class_1_prob)\n",
    "        new_violence_class_2_probs.append(avg_class_2_prob)\n",
    "    # If there are 200 or more rows, select the five rows with the lowest Class 0 probabilities\n",
    "    else:\n",
    "        selected_rows = group.nsmallest(5, 'Violence Class 0 Probability')\n",
    "        avg_class_0_prob = selected_rows['Violence Class 0 Probability'].mean()\n",
    "        avg_class_1_prob = selected_rows['Violence Class 1 Probability'].mean()\n",
    "        avg_class_2_prob = selected_rows['Violence Class 2 Probability'].mean()\n",
    "        new_ids.append(id)\n",
    "        new_texts.append(selected_rows['Text'].values[0])  # Select the text from the first row\n",
    "        new_violence_class_0_probs.append(avg_class_0_prob)\n",
    "        new_violence_class_1_probs.append(avg_class_1_prob)\n",
    "        new_violence_class_2_probs.append(avg_class_2_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame containing the original unprocessed data but with the representative classification scores\n",
    "processed_data = pd.DataFrame({\n",
    "    'ID': new_ids,\n",
    "    'Date': unprocessed_data['Date'],\n",
    "    'Fear Class 0 Probability': new_fear_class_0_probs,\n",
    "    'Fear Class 1 Probability': new_fear_class_1_probs,\n",
    "    'Fear Class 2 Probability': new_fear_class_2_probs,\n",
    "    'Violence Class 0 Probability': new_violence_class_0_probs,\n",
    "    'Violence Class 1 Probability': new_violence_class_1_probs,\n",
    "    'Violence Class 2 Probability': new_violence_class_2_probs,\n",
    "    'Document Type': unprocessed_data['Document Type'],\n",
    "    'Source': unprocessed_data['Source'],\n",
    "    'Text': unprocessed_data['Text'],\n",
    "    'PDF Indicator': unprocessed_data['PDF Indicator'],\n",
    "    'Content Page': unprocessed_data['Content Page'],\n",
    "    'PDF Link': unprocessed_data['PDF Link']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify articles as wholes without stopwords. This abandons the sentences approach and just looks at non-stopwords across the entire article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store probabilities, texts, and IDs for both classifiers\n",
    "probabilities_fear_list_2 = []\n",
    "probabilities_violence_list_2 = []\n",
    "texts_list_2 = []\n",
    "ids_list_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_entries_2 = unprocessed_data['Text'].tolist()\n",
    "ids_2 = unprocessed_data['Index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts: 100%|██████████| 5/5 [00:08<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "for text, text_id in tqdm(zip(text_entries_2, ids_2), total=len(text_entries_2), desc=\"Processing Texts\"):\n",
    "    # Lowercase the entire text\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Tokenize and remove stop words\n",
    "    tokens = nltk.word_tokenize(text_lower)\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum() and word not in stop_words_swedish]\n",
    "\n",
    "    # Reconstruct the text\n",
    "    text_filtered = ' '.join(tokens_filtered)\n",
    "\n",
    "    # Process the entire text at once\n",
    "    inputs = tokenizer(text_filtered, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    # Calculate probabilities for classifier_fear\n",
    "    outputs_fear = classifier_fear(**inputs)\n",
    "    probabilities_fear = torch.nn.functional.softmax(outputs_fear.logits, dim=1).tolist()[0]\n",
    "    probabilities_fear_list_2.append(probabilities_fear)\n",
    "\n",
    "    # Calculate probabilities for classifier_violence\n",
    "    outputs_violence = classifier_violence(**inputs)\n",
    "    probabilities_violence = torch.nn.functional.softmax(outputs_violence.logits, dim=1).tolist()[0]\n",
    "    probabilities_violence_list_2.append(probabilities_violence)\n",
    "\n",
    "    texts_list_2.append(text_filtered)\n",
    "    ids_list_2.append(text_id)\n",
    "\n",
    "# Now, probabilities_fear_list_2 and probabilities_violence_list_2 contain probabilities\n",
    "# for classifier_fear and classifier_violence respectively, for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the classified_sentence_data DataFrame\n",
    "classified_sentence_data_2 = pd.DataFrame({\n",
    "    'Fear Class 0 Probability_2': [item[0] for item in probabilities_fear_list_2],\n",
    "    'Fear Class 1 Probability_2': [item[1] for item in probabilities_fear_list_2],\n",
    "    'Fear Class 2 Probability_2': [item[2] for item in probabilities_fear_list_2],\n",
    "    'Violence Class 0 Probability_2': [item[0] for item in probabilities_violence_list_2],\n",
    "    'Violence Class 1 Probability_2': [item[1] for item in probabilities_violence_list_2],\n",
    "    'Violence Class 2 Probability_2': [item[2] for item in probabilities_violence_list_2],\n",
    "    'Text_2': texts_list_2,\n",
    "    'ID_2': ids_list_2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fear Class 0 Probability_2</th>\n",
       "      <th>Fear Class 1 Probability_2</th>\n",
       "      <th>Fear Class 2 Probability_2</th>\n",
       "      <th>Violence Class 0 Probability_2</th>\n",
       "      <th>Violence Class 1 Probability_2</th>\n",
       "      <th>Violence Class 2 Probability_2</th>\n",
       "      <th>Text_2</th>\n",
       "      <th>ID_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.976913</td>\n",
       "      <td>0.018334</td>\n",
       "      <td>0.004753</td>\n",
       "      <td>0.996448</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>2023 utbildningsdepartementet utbildningsminis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.943017</td>\n",
       "      <td>0.040467</td>\n",
       "      <td>0.016517</td>\n",
       "      <td>0.996085</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>regeringsbeslut 1 2023 klimat näringslivsdepar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.985671</td>\n",
       "      <td>0.010385</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.996642</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>remiss 2023 arbetsmarknadsdepartementet enhete...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.990124</td>\n",
       "      <td>0.007137</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.997457</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>remiss 2023 01488 försvarsdepartementet enhete...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.984264</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>0.004507</td>\n",
       "      <td>0.997125</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>remiss 2023 reviderat socialdepartementet enhe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fear Class 0 Probability_2  Fear Class 1 Probability_2  \\\n",
       "0                    0.976913                    0.018334   \n",
       "1                    0.943017                    0.040467   \n",
       "2                    0.985671                    0.010385   \n",
       "3                    0.990124                    0.007137   \n",
       "4                    0.984264                    0.011228   \n",
       "\n",
       "   Fear Class 2 Probability_2  Violence Class 0 Probability_2  \\\n",
       "0                    0.004753                        0.996448   \n",
       "1                    0.016517                        0.996085   \n",
       "2                    0.003944                        0.996642   \n",
       "3                    0.002739                        0.997457   \n",
       "4                    0.004507                        0.997125   \n",
       "\n",
       "   Violence Class 1 Probability_2  Violence Class 2 Probability_2  \\\n",
       "0                        0.001466                        0.002086   \n",
       "1                        0.002019                        0.001896   \n",
       "2                        0.001547                        0.001811   \n",
       "3                        0.001298                        0.001245   \n",
       "4                        0.001188                        0.001687   \n",
       "\n",
       "                                              Text_2  ID_2  \n",
       "0  2023 utbildningsdepartementet utbildningsminis...     0  \n",
       "1  regeringsbeslut 1 2023 klimat näringslivsdepar...     1  \n",
       "2  remiss 2023 arbetsmarknadsdepartementet enhete...     2  \n",
       "3  remiss 2023 01488 försvarsdepartementet enhete...     3  \n",
       "4  remiss 2023 reviderat socialdepartementet enhe...     4  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_sentence_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the whole-text classifications and processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be joined from classified_sentence_data_2\n",
    "columns_to_join = ['ID_2', 'Fear Class 0 Probability_2', 'Fear Class 1 Probability_2', 'Fear Class 2 Probability_2',\n",
    "                    'Violence Class 0 Probability_2', 'Violence Class 1 Probability_2', 'Violence Class 2 Probability_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge processed_data with the selected columns from classified_sentence_data_2\n",
    "fully_classified_data = pd.merge(processed_data, classified_sentence_data_2[columns_to_join], how='left', left_on='ID', right_on='ID_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the redundant 'ID_2' column\n",
    "fully_classified_data.drop(columns=['ID_2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up and organize the data in fully_classified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename probability columns\n",
    "fully_classified_data.rename(columns={\n",
    "    'Fear Class 0 Probability': 'Fear 0 Sen',\n",
    "    'Fear Class 1 Probability': 'Fear 1 Sen',\n",
    "    'Fear Class 2 Probability': 'Fear 2 Sen',\n",
    "    'Fear Class 0 Probability_2': 'Fear 0 Whole',\n",
    "    'Fear Class 1 Probability_2': 'Fear 1 Whole',\n",
    "    'Fear Class 2 Probability_2': 'Fear 2 Whole',\n",
    "    'Violence Class 0 Probability_2': 'Violence 0 Whole',\n",
    "    'Violence Class 1 Probability_2': 'Violence 1 Whole',\n",
    "    'Violence Class 2 Probability_2': 'Violence 2 Whole',\n",
    "    'Violence Class 0 Probability': 'Violence 0 Sen',\n",
    "    'Violence Class 1 Probability': 'Violence 1 Sen',\n",
    "    'Violence Class 2 Probability': 'Violence 2 Sen'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combined list of Swedish words related to immigration, integration, assimilation, Middle Eastern cultures, and languages\n",
    "keywords = [\n",
    "    \"Invandring\", \"Migrationspolitik\", \"Asylsökande\", \"Flyktingar\", \"Immigrant\", \"Utvandring\",\n",
    "    \"Integration\", \"Integrationspolitik\", \"Mångkultur\", \"Integrationstjänster\", \"Integrationssvårigheter\", \"Integrationsprocess\",\n",
    "    \"Assimilation\", \"Anpassning\", \"Kulturell assimilering\", \"Kulturell anpassning\", \"Språklig assimilering\", \"Social assimilering\",\n",
    "    \"Arabisk\", \"Syrisk\", \"Irakisk\", \"Iransk\", \"Palestinsk\", \"Libanesisk\", \"Turkisk\", \"Kurdisk\", \"Persisk\",\n",
    "    \"Araber\", \"Syrier\", \"Irakier\", \"Iranier\", \"Palestinier\", \"Libaneser\", \"Turkar\", \"Kurder\"\n",
    "]\n",
    "\n",
    "# Additional words related to immigration, integration, refugees, migration, and assimilation\n",
    "additional_keywords = [\n",
    "    \"Invandring\", \"Integration\", \"Flykting\", \"Asyl\", \"Migrationsverket\", \"Anhöriginvandring\", \"Utlänning\", \n",
    "    \"Samhällsintegration\", \"Språkundervisning\", \"Mångfald\", \"Tolerans\", \"Diskriminering\", \"Rasism\", \"Inkludering\", \n",
    "    \"Immigrationslagar\", \"Gränskontroll\", \"Upphållstillstånd\", \"Integrationspolitik\", \n",
    "    \"Skyddsbehövande\", \"Internflykting\", \"Utvisning\", \"Assimilering\", \"Återvandring\", \n",
    "    \"Anpassning\", \"Kulturkrock\", \"Etnicitet\", \"Terrorism\", \"Muslim\", \"Islam\", \"Segregation\", \"Assimilation\",\n",
    "    \"Syrien\", \"Iran\", \"Turkiet\", \"Irak\", \"Palestina\", \"Libanon\", \"Mellanöstern\"\n",
    "]\n",
    "\n",
    "# Remove duplicates and add additional_keywords to the original list\n",
    "keywords = list(set(keywords + additional_keywords))\n",
    "\n",
    "# Dictionary to store keyword frequencies\n",
    "keyword_frequencies = {keyword: [] for keyword in keywords}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummies for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add keywords to a df called processed_data_keyword_coded that combines keyword binary with the fully_classified_data\n",
    "for keyword in keywords:\n",
    "    # Iterate through each keyword and check its presence in each text entry\n",
    "    keyword_occurrences = fully_classified_data['Text'].str.contains(keyword, case=False, na=False)\n",
    "    keyword_frequencies[keyword] = keyword_occurrences.astype(int)\n",
    "\n",
    "# Create a new DataFrame to store the keyword frequencies\n",
    "keyword_df = pd.DataFrame(keyword_frequencies)\n",
    "\n",
    "# Concatenate the keyword frequencies DataFrame with the original DataFrame\n",
    "processed_data_keyword_coded = pd.concat([fully_classified_data, keyword_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Months and Years for processed_data_with_keywords\n",
    "# Custom mapping for Swedish month names to English month names\n",
    "month_mapping = {\n",
    "    'januari': 'January',\n",
    "    'februari': 'February',\n",
    "    'mars': 'March',\n",
    "    'april': 'April',\n",
    "    'maj': 'May',\n",
    "    'juni': 'June',\n",
    "    'juli': 'July',\n",
    "    'augusti': 'August',\n",
    "    'september': 'September',\n",
    "    'oktober': 'October',\n",
    "    'november': 'November',\n",
    "    'december': 'December'\n",
    "}\n",
    "\n",
    "# Function to convert Swedish month names to English\n",
    "def convert_swedish_to_english(date_string):\n",
    "    day, month, year = date_string.split(' ')\n",
    "    month = month_mapping[month.lower()]\n",
    "    return f\"{day} {month} {year}\"\n",
    "\n",
    "# Convert Times on processed_data_keyword_coded\n",
    "# Apply the conversion function to the 'Date' column\n",
    "processed_data_keyword_coded['Date'] = processed_data_keyword_coded['Date'].apply(convert_swedish_to_english)\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "processed_data_keyword_coded['Date'] = pd.to_datetime(processed_data_keyword_coded['Date'], format='%d %B %Y')\n",
    "\n",
    "# Extract month and year into new columns\n",
    "processed_data_keyword_coded['Month'] = processed_data_keyword_coded['Date'].dt.month\n",
    "processed_data_keyword_coded['Year'] = processed_data_keyword_coded['Date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark content with keywords present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each row for the presence of any keywords; if one is there, make 'Keyword Present' into 1; otherwise make it 0\n",
    "processed_data_keyword_coded['Keyword Present'] = processed_data_keyword_coded[keywords].any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a collection of the keywords in each text to each row of processed_data_keyword_coded\n",
    "processed_data_keyword_coded['Keywords'] = processed_data_keyword_coded['Text'].apply(lambda text: [keyword for keyword in keywords if keyword.lower() in text.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate F-V Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-based calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F-V Score for each row of the processed_data_keyword_coded data\n",
    "processed_data_keyword_coded['Sentence FVS'] = (1 - ((processed_data_keyword_coded['Fear 0 Sen'] + processed_data_keyword_coded['Violence 0 Sen']) / 2)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article-based calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F-V Score for each row of the processed_data_keyword_coded data\n",
    "processed_data_keyword_coded['Article FVS'] = (1 - ((processed_data_keyword_coded['Fear 0 Whole'] + processed_data_keyword_coded['Violence 0 Whole']) / 2)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged calcuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F-V Score for each row of the processed_data_keyword_coded data\n",
    "processed_data_keyword_coded['Average FVS'] = (processed_data_keyword_coded['Sentence FVS'] + processed_data_keyword_coded['Article FVS']) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to CSV\n",
    "processed_data_keyword_coded.to_csv(output_file_name, index=False)\n",
    "\n",
    "print(f\"Processed data has been saved to {output_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
