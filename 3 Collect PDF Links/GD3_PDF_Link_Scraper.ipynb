{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries and Set Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "from random import randint\n",
    "\n",
    "# Other Settings\n",
    "pd.set_option('display.max.colwidth', None) # max display width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file into a DataFrame\n",
    "csv_file_path = \"publication_info.csv\"  # Update with the correct file path\n",
    "existing_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Rename the columns to match your desired names\n",
    "existing_data.columns = ['Content Links', 'Publishing Dates']\n",
    "\n",
    "# Append the loaded data to the existing DataFrame\n",
    "article_link_directory = existing_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset the data for quick-time tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_article_link_directory = article_link_directory.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLECT THE PDF LINKS FROM EACH CONTENT PAGE CONTAINING A PDF; IF A CONTENT PAGE CONTAINS NO PDF, GRAB ALL TEXT INSTEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step 1: Collecting PDF Links: 100%|██████████| 30/30 [01:06<00:00,  2.22s/link]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to collect the links\n",
    "layered_links = []\n",
    "text_data_list = []\n",
    "\n",
    "# No content alert\n",
    "no_content_alert = \"NO CONTENT\"\n",
    "\n",
    "for i, link in enumerate(tqdm(practice_article_link_directory['Content Links'], desc=\"Step 1: Collecting PDF Links\", unit=\"link\")):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(link)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find the first instance of <ul class=\"list--Block--icons\">\n",
    "            ul_tag = soup.find('ul', class_='list--Block--icons')\n",
    "\n",
    "            # Check if the <ul> tag is found\n",
    "            if ul_tag:\n",
    "                # Find the first <a> tag with href inside the <ul> tag\n",
    "                first_link = ul_tag.find('a', href=True)\n",
    "                # no_content_alert = \"NO CONTENT\"\n",
    "                if first_link:\n",
    "                    layered_links.append(first_link['href'])\n",
    "                    text_data_list.append(no_content_alert)\n",
    "                else:\n",
    "                    layered_links.append(no_content_alert)\n",
    "            else:\n",
    "                # If <ul> tag is not found, extract text from <p> tags\n",
    "                paragraphs = soup.find_all('p')\n",
    "                text_data = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
    "                text_data_list.append(text_data)\n",
    "                layered_links.append(no_content_alert)\n",
    "        else:\n",
    "            print(f\"Failed to fetch URL: {link}, Status code: {response.status_code}\")\n",
    "            # If <ul> tag is not found, extract text from <p> tags\n",
    "            paragraphs = soup.find_all('p')\n",
    "            text_data = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
    "            text_data_list.append(text_data)\n",
    "            layered_links.append(no_content_alert)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing URL: {link}, Error: {str(e)}\")\n",
    "        text_data_list.append(no_content_alert)\n",
    "        layered_links.append(no_content_alert)\n",
    "\n",
    "    # Introduce a random delay time before the next request\n",
    "    time.sleep(1)\n",
    "    # time.sleep(randint(1, 2))  # Adjust the delay time as needed\n",
    "\n",
    "    # Add a break statement if the loop index exceeds the expected number of links\n",
    "    if i + 1 > len(practice_article_link_directory['Content Links']):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe to hold scraped text information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the lists and rename columns\n",
    "additional_data = pd.DataFrame({'Collected Links': layered_links, 'Outside PDF Text': text_data_list})\n",
    "# Concatenate the new DataFrame with the original DataFrame along the columns axis (axis=1)\n",
    "practice_article_link_directory = pd.concat([practice_article_link_directory, additional_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add prefix to links to make valid links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prefix to the links in the result_df to complete the links\n",
    "# Define the prefix to add\n",
    "prefix = 'https://www.regeringen.se'\n",
    "\n",
    "# Define a function to conditionally add the prefix\n",
    "def add_prefix(link):\n",
    "    if link == 'NO CONTENT':\n",
    "        return link\n",
    "    else:\n",
    "        return f'{prefix}{link}'\n",
    "\n",
    "# Use the .apply() method with the defined function to add the prefix conditionally\n",
    "practice_article_link_directory['Full Collected Links'] = practice_article_link_directory['Collected Links'].apply(add_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove cookies warning from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the repetitive string in front of all the rows on the Outside PDF Text\n",
    "# Define the string to remove\n",
    "string_to_remove = \"På regeringen.se använder vi kakor för att löpande förbättra webbplatsen. Du väljer själv om du accepterar kakor.Läs om kakor\\nHuvudnavigering\\nHuvudnavigering\\n\"\n",
    "\n",
    "# Define a function to conditionally remove the specified string\n",
    "def remove_string(text):\n",
    "    if text == 'NO CONTENT':\n",
    "        return text\n",
    "    else:\n",
    "        return text.replace(string_to_remove, '', 1)  # Remove the specified string only from the beginning\n",
    "\n",
    "# Apply the defined function to 'Outside PDF Text' column\n",
    "practice_article_link_directory['Outside PDF Text'] = practice_article_link_directory['Outside PDF Text'].apply(remove_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download practice_article_link_directory as a fail safe\n",
    "practice_article_link_directory.to_csv('practice_article_link_directory.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanh\\AppData\\Local\\Temp\\ipykernel_9464\\1675699489.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  practice_article_link_directory['Collected Links'] = \"\"\n",
      "C:\\Users\\ryanh\\AppData\\Local\\Temp\\ipykernel_9464\\1675699489.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  practice_article_link_directory['Outside PDF Text'] = \"\"\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# No content alert\n",
    "no_content_alert = \"NO CONTENT\"\n",
    "\n",
    "# Add new columns to the DataFrame\n",
    "practice_article_link_directory['Collected Links'] = \"\"\n",
    "practice_article_link_directory['Outside PDF Text'] = \"\"\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file_path = \"output_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step 1: Collecting PDF Links:  97%|█████████▋| 29/30 [01:16<00:02,  2.64s/link]\n"
     ]
    }
   ],
   "source": [
    "# Open the CSV file in write mode with a CSV writer\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write the header row to the CSV file\n",
    "    csv_writer.writerow([\"Collected Links\", \"Outside PDF Text\"])\n",
    "\n",
    "    for i, link in enumerate(tqdm(practice_article_link_directory['Content Links'], desc=\"Step 1: Collecting PDF Links\", unit=\"link\")):\n",
    "        try:\n",
    "            # Send an HTTP GET request to the URL\n",
    "            response = requests.get(link)\n",
    "\n",
    "            # Check if the request was successful (status code 200)\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content of the page\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Find the first instance of <ul class=\"list--Block--icons\">\n",
    "                ul_tag = soup.find('ul', class_='list--Block--icons')\n",
    "\n",
    "                # Check if the <ul> tag is found\n",
    "                if ul_tag:\n",
    "                    # Find the first <a> tag with href inside the <ul> tag\n",
    "                    first_link = ul_tag.find('a', href=True)\n",
    "                    if first_link:\n",
    "                        practice_article_link_directory.at[i, 'Collected Links'] = first_link['href']\n",
    "                        practice_article_link_directory.at[i, 'Outside PDF Text'] = no_content_alert\n",
    "                    else:\n",
    "                        practice_article_link_directory.at[i, 'Collected Links'] = no_content_alert\n",
    "                else:\n",
    "                    # If <ul> tag is not found, extract text from <p> tags\n",
    "                    paragraphs = soup.find_all('p')\n",
    "                    text_data = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
    "                    practice_article_link_directory.at[i, 'Outside PDF Text'] = text_data\n",
    "                    practice_article_link_directory.at[i, 'Collected Links'] = no_content_alert\n",
    "            else:\n",
    "                print(f\"Failed to fetch URL: {link}, Status code: {response.status_code}\")\n",
    "                # If <ul> tag is not found, extract text from <p> tags\n",
    "                paragraphs = soup.find_all('p')\n",
    "                text_data = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
    "                practice_article_link_directory.at[i, 'Outside PDF Text'] = text_data\n",
    "                practice_article_link_directory.at[i, 'Collected Links'] = no_content_alert\n",
    "\n",
    "            # Write the data to the CSV file in each iteration\n",
    "            csv_writer.writerow([practice_article_link_directory.at[i, 'Collected Links'],\n",
    "                                 practice_article_link_directory.at[i, 'Outside PDF Text']])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing URL: {link}, Error: {str(e)}\")\n",
    "            # If an error occurs, write the available data to the CSV file before continuing\n",
    "            csv_writer.writerow([no_content_alert, no_content_alert])\n",
    "\n",
    "        # Introduce a random delay time before the next request\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Add a break statement if the loop index is equal to the expected number of links minus 1\n",
    "        if i == len(practice_article_link_directory['Content Links']) - 1:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
